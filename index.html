<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhuoyang Liu</title>

    <meta name="author" content="Zhuoyang Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zhuoyang Liu
                </p>
                <p>
		I'm a third-year undergraduate student at <a href="https://english.pku.edu.cn/">Peking University</a>, majoring in Computer Science.
    Currently, I am a visiting student at <a href="https://bair.berkeley.edu/">BAIR</a> at <a href="https://www.berkeley.edu/">UC Berkeley</a>.
    Prior to this, I was a research intern at the <a href="https://pku-hmi-lab.github.io/HMI-Web/index.html#">HMI Lab</a> at Peking University, advised by Prof. <a href="https://cs.pku.edu.cn/info/1236/2067.htm">Shanghang Zhang</a>.
    My research interests lie in the application of multimodal large models, specifically VLA models, in robot manipulation.
                </p>
                <p style="text-align:center">
                  <a href="zhuoyangliu6@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Zhuoyang-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=WfZvYgoAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/liu_zhuoyang13">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ZhuoyangLiu2005">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Stanford1-min.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Stanford1-min.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in Computer Vision, Robot Learning and Embodied Large Multimodal Models. My research focuses on how to enhance large multimodal models to better reason about the physical world and develop effective task planning. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


<tr onmouseout="last0_stop()" onmouseover="last0_start()" bgcolor="#ffffd0"> 
  <td style="padding:16px;width:20%;vertical-align:middle"> 
    <div class="one">
      <div class="two" id='last0_image' style="opacity: 1;"> 
        <img src='images/last0_teaser.png' width=100%>
      </div>
      <video width=100% height=100% muted autoplay loop>
        <source src="images/last0_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    <script type="text/javascript">
      function last0_start() {
        document.getElementById('last0_image').style.opacity = "0";
      }

      function last0_stop() {
        document.getElementById('last0_image').style.opacity = "1";
      }
      last0_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://vla-last0.github.io/">
      <span class="papertitle">LaST<sub>0</sub>: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</span>
    </a>
    <br>
    <strong>Zhuoyang Liu<sup>*</sup></strong>, <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>*</sup></a>,
    <a href="https://scholar.google.com.hk/citations?user=tT03tysAAAAJ&hl=zh-CN&oi=sra">Hao Chen<sup>*</sup></a>,
    <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=2Uxuso4AAAAJ">Jiale Yu</a>, 
    <a href="https://ziyuguo99.github.io/">Ziyu Guo</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=iXENYMYAAAAJ">Chengkai Hou</a>, <br>
    <a href="http://guchenyang.site/">Chenyang Gu</a>, 
    <a href="https://scholar.google.com/citations?user=ICCEv6YAAAAJ&hl=zh-CN&oi=ao">Xiangju Mi</a>, 
    <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>,
    <a href="https://scholar.google.com/citations?user=iqbx6RQAAAAJ&hl=en">Kun Wu</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=f6uvd6kAAAAJ">Zhengping Che</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=IirM9zMAAAAJ">Jian Tang</a>, <br>
    <a href="https://scholar.google.com/citations?user=OFdytjoAAAAJ&hl=en">Pheng-Ann Heng</a>,
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>
    <br>
    <em>arXiv</em>, 2026
    <br>
    <a href="https://vla-last0.github.io/">project page</a>
    /
    <a href="https://arxiv.org/abs/2601.05248v2">arXiv</a>
    <p></p>
    <p>
      A VLA model that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize.
    </p>
  </td>
</tr>


<tr onmouseout="manualvla_stop()" onmouseover="manualvla_start()">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='manualvla_image' style="opacity: 1;">
        <img src='images/manualvla_teaser.png' width=100%>
      </div>
      <video width=100% height=100% muted autoplay loop>
        <source src="images/manualvla_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    <script type="text/javascript">
      function manualvla_start() {
        document.getElementById('manualvla_image').style.opacity = "0";
      }

      function manualvla_stop() {
        document.getElementById('manualvla_image').style.opacity = "1";
      }
      manualvla_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://sites.google.com/view/maunalvla/">
      <span class="papertitle">ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</span>
    </a>
    <br>
    <a href="http://guchenyang.site/">Chenyang Gu<sup>*</sup></a>, 
    <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>*</sup></a>,
    <a href="https://scholar.google.com.hk/citations?user=tT03tysAAAAJ&hl=zh-CN&oi=sra">Hao Chen<sup>*</sup></a>, 
    <a href="">Runzhong Huang<sup>*</sup></a>,   
    <a href="https://github.com/qingpowuwu">Qingpo Wuwu</a>,
    <strong>Zhuoyang Liu</strong>, <br>
    <a href="https://clorislili.github.io/clorisLi/">Xiaoqi Li</a>, 
    Ying Li, 
    <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>, 
    Peng Jia, 
    <a href="https://scholar.google.com/citations?user=OFdytjoAAAAJ&hl=en">Pheng-Ann Heng</a>,
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>
    <br>
    <em>arXiv</em>, 2025
    <br>
    <a href="https://sites.google.com/view/maunalvla/">project page</a>
    /
    <a href="https://arxiv.org/abs/2512.02013">arXiv</a>
    /
    <a href="https://www.youtube.com/watch?v=FbJnfDq8XKo&t=1s">video</a>
    <p></p>
    <p>
      A unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution.
    </p>
  </td>
</tr>


<tr onmouseout="dualvla_stop()" onmouseover="dualvla_start()" bgcolor="#ffffd0">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='dualvla_image'>
        <img src='images/dualvla_teaser.png' width=100%>
      </div>
      <img src='images/dualvla_method.png' width=100%>
    </div>
    <script type="text/javascript">
      function dualvla_start() {
        document.getElementById('dualvla_image').style.opacity = "0";
      }
      function dualvla_stop() {
        document.getElementById('dualvla_image').style.opacity = "1";
      }
      dualvla_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://costaliya.github.io/DualVLA/">
      <span class="papertitle">DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action</span>
    </a>
    <br>
    <a href="https://costaliya.github.io/">Zhen Fang<sup>*</sup></a>, 
    <strong>Zhuoyang Liu<sup>*</sup></strong>, 
    <a href="https://liujiaming1996.github.io/">Jiaming Liu</a>,
    <a href="https://scholar.google.com.hk/citations?user=tT03tysAAAAJ&hl=zh-CN&oi=sra">Hao Chen</a>, 
    <a href="https://scholar.google.com.hk/citations?user=XJmAr8EAAAAJ&hl=zh-CN">Yu Zeng</a>,   
    Shiting Huang, <br>
    <a href="https://lovesnowbest.site/">Zehui Chen</a>, 
    <a href="https://lin-chen.site/">Lin Chen</a>, 
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>,
    <a href="https://scholar.google.co.uk/citations?user=r6CvuOUAAAAJ&hl=en">Feng Zhao</a>
    <br>
    <em>arXiv</em>, 2025
    <br>
    <a href="https://costaliya.github.io/DualVLA/">project page</a>
    /
    <a href="https://arxiv.org/abs/2511.22134">arXiv</a>
    <p></p>
    <p>
      DualVLA improves action performance through carefully designed post-training while preserving the reasoning ability.
    </p>
  </td>
</tr>


<tr onmouseout="mla_stop()" onmouseover="mla_start()" bgcolor="#ffffd0">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='mla_image' style="opacity: 1;">
        <img src='images/mla_teaser.png' width=100%>
      </div>
      <video width=100% height=100% muted autoplay loop>
        <source src="images/mla_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    <script type="text/javascript">
      function mla_start() {
        document.getElementById('mla_image').style.opacity = "0";
      }

      function mla_stop() {
        document.getElementById('mla_image').style.opacity = "1"; 
      }
      mla_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://sites.google.com/view/open-mla">
      <span class="papertitle">MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation</span>
    </a>
    <br>
    <strong>Zhuoyang Liu<sup>*</sup></strong>, 
    <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>*</sup></a>,
    <a href="https://scholar.google.com/citations?hl=en&user=i7XUj24AAAAJ">Jiadong Xu</a>,
    <a href="">Nuowei Han</a>,
    <a href="http://guchenyang.site/">Chenyang Gu</a>, 
    <a href="https://scholar.google.com.hk/citations?user=tT03tysAAAAJ&hl=zh-CN&oi=sra">Hao Chen</a>, <br>
    <a href="https://kaichen-z.github.io/">Kaichen Zhou</a>,
    <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>, 
    <a href="">Kai Chin Hsieh</a>,
    <a href="https://scholar.google.com/citations?user=iqbx6RQAAAAJ&hl=en">Kun Wu</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=f6uvd6kAAAAJ">Zhengping Che</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=IirM9zMAAAAJ">Jian Tang</a>, <br>
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>
    <br>
    <em>ICRA</em>, 2026
    <br>
    <a href="https://sites.google.com/view/open-mla">project page</a>
    /
    <a href="https://arxiv.org/abs/2509.26642">arXiv</a>
    /
    <a href="https://github.com/ZhuoyangLiu2005/MLA">code</a>
    <p></p>
    <p>
      A multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling.
    </p>
  </td>
</tr>


<tr onmouseout="robomind2_stop()" onmouseover="robomind2_start()">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='robomind2_image'>
        <img src='images/robomind2_teaser.png' width=100%>
      </div>
      <img src='images/robomind2_method.png' width=100%>
    </div>
    <script type="text/javascript">
      function robomind2_start() {
        document.getElementById('robomind2_image').style.opacity = "0";
      }
      function robomind2_stop() {
        document.getElementById('robomind2_image').style.opacity = "1";
      }
      robomind2_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://x-humanoid-robomind.github.io/">
      <span class="papertitle">RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?hl=en&user=iXENYMYAAAAJ">Chengkai Hou<sup>*</sup></a>, 
    <a href="https://scholar.google.com/citations?user=iqbx6RQAAAAJ&hl=en">Kun Wu<sup>*</sup></a>, 
    <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>*</sup></a>,
    <a href="https://scholar.google.com/citations?hl=en&user=f6uvd6kAAAAJ">Zhengping Che<sup>*</sup></a>, 
    <a href="">Di Wu<sup>*</sup></a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=c8elkNoAAAAJ">Fei Liao<sup>*</sup></a>, 
    <a href="">Guangrun Li<sup>*</sup></a>, <br>
    <a href="https://scholar.google.com/citations?hl=en&user=6IVnh00AAAAJ"> Jingyang He<sup>*</sup></a>, 
    <a href=""> Qiuxuan Feng<sup>*</sup></a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=gFW4G98AAAAJ">Zhao Jin<sup>*</sup></a>, 
    <a href="http://guchenyang.site/">Chenyang Gu</a>, 
    <strong>Zhuoyang Liu</strong>, 
    <a href="">Nuowei Han</a>, <br>
    <a href="https://scholar.google.com/citations?user=ICCEv6YAAAAJ&hl=zh-CN&oi=ao">Xiangju Mi</a>, 
    <a href="">Yaoxu Lyu</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=NZSt0F8AAAAJ">Yankai Fu</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=2Of6xZUAAAAJ">Gaole Dai</a>, 
    <a href="">Langzhe Gu</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=DO8btgUAAAAJ">Tao Li</a>, 
    <a href="">Yuheng Zhang</a>, <br>
    <a href="">Yixue Zhang</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=-DLpF8EAAAAJ">Xinhua Wang</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=78zByQYAAAAJ">Shichao Fan</a>, 
    <a href="">Meng Li</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=bGU67mYAAAAJ">Zhen Zhao</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=OFOJM5MAAAAJ">Ning Liu</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=jKHMVnYAAAAJ">Zhiyuan Xu</a>, <br>
    <a href="https://scholar.google.com/citations?hl=en&user=ucklnZsAAAAJ">Pei Ren</a>, 
    <a href="">Junjie Ji</a>, 
    <a href="">Haonan Liu</a>,
    <a href="">Kuan Cheng</a>, 
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=IirM9zMAAAAJ">Jian Tang</a>
    <br>
    <em>arXiv</em>, 2025
    <br>
    <a href="https://x-humanoid-robomind.github.io/">project page</a>
    /
    <a href="https://arxiv.org/abs/2512.24653">arXiv</a>
    <p></p>
    <p>
      RoboMIND 2.0 is a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks.
    </p>
  </td>
</tr>


<tr onmouseout="acdit_stop()" onmouseover="acdit_start()" >
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='acdit_image' style="opacity: 1;">
        <img src='images/acdit_teaser.png' width=100%>
      </div>
      <video width=100% height=100% muted autoplay loop>
        <source src="images/acdit_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    <script type="text/javascript">
      function acdit_start() {
        document.getElementById('acdit_image').style.opacity = "0";
      }
      function acdit_stop() {
        document.getElementById('acdit_image').style.opacity = "1";
      }
      acdit_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://ac-dit.github.io/">
      <span class="papertitle">AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation</span>
    </a>
    <br>
    <a href="https://cscsx.github.io/">Sixiang Chen<sup>*</sup></a>, 
    <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>*</sup></a>,
    <a href="https://siriyep.github.io/">Siyuan Qian<sup>*</sup></a>,
    <a href="">Han Jiang</a>,
    <a href="https://clorislili.github.io/clorisLi/">Xiaoqi Li</a>, 
    <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>, </br>
    <strong>Zhuoyang Liu</strong>, 
    <a href="http://guchenyang.site/">Chenyang Gu</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=iXENYMYAAAAJ">Chengkai Hou</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=2xR6P5AAAAAJ">Pengwei Wang</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=4XVJrRAAAAAJ">Zhongyuan Wang</a>,
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>
    <br>
    <em>NeurIPS</em>, 2025
    <br>
    <a href="https://ac-dit.github.io/">project page</a>
    /
    <a href="https://arxiv.org/abs/2507.01961">arXiv</a>
    /
    <a href="https://github.com/PKU-HMI-Lab/AC-DiT">code</a>
    <p></p>
    <p>
      Adaptive Coordination Diffusion Transformer (AC-DiT) enhances mobile base and manipulator coordination for end-to-end mobile manipulation.
    </p>
  </td>
</tr>


<tr onmouseout="fis_stop()" onmouseover="fis_start()" bgcolor="#ffffd0">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='fis_image' style="opacity: 1;">
        <img src='images/fis_teaser.png' width=100%>
      </div>
      <video width=100% height=100% muted autoplay loop>
        <source src="images/fis_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    <script type="text/javascript">
      function fis_start() {
        document.getElementById('fis_image').style.opacity = "0";
      }
      function fis_stop() {
        document.getElementById('fis_image').style.opacity = "1";
      }
      fis_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://fast-in-slow.github.io/">
      <span class="papertitle">Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning</span>
    </a>
    <br>
    <a href="https://scholar.google.com.hk/citations?user=tT03tysAAAAJ&hl=zh-CN&oi=sra">Hao Chen<sup>*</sup></a>, 
    <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>*</sup></a>,
    <a href="http://guchenyang.site/">Chenyang Gu<sup>*</sup></a>, 
    <strong>Zhuoyang Liu<sup>*</sup></strong>, 
    <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>,
    <a href="https://clorislili.github.io/clorisLi/">Xiaoqi Li</a>, </br>
    <a href="https://scholar.google.com/citations?hl=en&user=z5hCQDYAAAAJ">Xiao He</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=fWDoWsQAAAAJ">Yandong Guo</a>, 
    <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing FU</a>, 
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>,
    <a href="https://scholar.google.com/citations?user=OFdytjoAAAAJ&hl=en">Pheng-Ann Heng</a>
    <br>
    <em>NeurIPS</em>, 2025
    <br>
    <a href="https://fast-in-slow.github.io/">project page</a>
    /
    <a href="https://arxiv.org/abs/2506.01953">arXiv</a>
    /
    <a href="https://github.com/CHEN-H01/Fast-in-Slow">code</a>
    <p></p>
    <p>
      Unlike previous dual-system VLA methods that attach a separate policy head as System 1, FiS-VLA repurposes the final transformer blocks of an intact VLM as System 1, while retaining the full model for System 2 reasoning.
    </p>
  </td>
</tr>


<tr onmouseout="threedsvla_stop()" onmouseover="threedsvla_start()" >
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='threedsvla_image'>
        <img src='images/threedsvla_teaser.png' width=100%>
      </div>
      <img src='images/threedsvla_method.png' width=100%>
    </div>
    <script type="text/javascript">
      function threedsvla_start() {
        document.getElementById('threedsvla_image').style.opacity = "0";
      }
      function threedsvla_stop() {
        document.getElementById('threedsvla_image').style.opacity = "1";
      }
      threedsvla_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://proceedings.mlr.press/v305/li25g.html">
      <span class="papertitle">3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation</span>
    </a>
    <br>
    <a href="https://clorislili.github.io/clorisLi/">Xiaoqi Li<sup>*</sup></a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=cW-kkGwAAAAJ">Liang Heng<sup>*</sup></a>,
    <a href="https://liujiaming1996.github.io/">Jiaming Liu</a>,
    <a href="https://sxy7147.github.io/">Yan Shen</a>, 
    <a href="http://guchenyang.site/">Chenyang Gu</a>,
    <strong>Zhuoyang Liu</strong>, <br>
    <a href="https://scholar.google.com.hk/citations?user=tT03tysAAAAJ&hl=zh-CN&oi=sra">Hao Chen</a>,
    <a href="">Nuowei Han</a>,
    <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>,
    <a href="https://ha0tang.github.io/">Hao Tang</a>, 
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>,
    <a href="https://zsdonghao.github.io/">Hao Dong</a>
    <br>
    <em>CoRL</em>, 2025
    <br>
    <a href="https://proceedings.mlr.press/v305/li25g.html">paper</a>
    <p></p>
    <p>
      3DS-VLA enhances pretrained 2D vision-language models (VLMs) with comprehensive 3D awareness, enabling the prediction of robust end-effector poses.
    </p>
  </td>
</tr>


<tr onmouseout="hybridvla_stop()" onmouseover="hybridvla_start()" bgcolor="#ffffd0">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='hybridvla_image' style="opacity: 1;">
        <img src='images/hybridvla_teaser.png' width=100%>
      </div>
      <video width=100% height=100% muted autoplay loop>
        <source src="images/hybridvla_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    <script type="text/javascript">
      function hybridvla_start() {
        document.getElementById('hybridvla_image').style.opacity = "0";
      }
      function hybridvla_stop() {
        document.getElementById('hybridvla_image').style.opacity = "1";
      }
      hybridvla_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://hybrid-vla.github.io/">
      <span class="papertitle">HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model</span>
    </a>
    <br>
    <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>*</sup></a>,
    <a href="https://scholar.google.com.hk/citations?user=tT03tysAAAAJ&hl=zh-CN&oi=sra">Hao Chen<sup>*</sup></a>, 
    <strong>Zhuoyang Liu<sup>*</sup></strong>, 
    <a href="https://scholar.google.com/citations?user=fUK2YhkAAAAJ&hl=zh-CN">Pengju An<sup>*</sup></a>, 
    <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>,
    <a href="http://guchenyang.site/">Chenyang Gu</a>, </br>
    <a href="https://clorislili.github.io/clorisLi/">Xiaoqi Li</a>, 
    <a href="https://ziyuguo99.github.io/">Ziyu Guo</a>,
    <a href="https://cscsx.github.io/">Sixiang Chen</a>, 
    <a href="https://scholar.google.com/citations?hl=zh-CN&user=AUVUNusAAAAJ">Mengzhen Liu</a>, 
    <a href="https://scholar.google.com/citations?hl=en&user=iXENYMYAAAAJ">Chengkai Hou</a>,
    <a href="https://scholar.google.com/citations?hl=zh-CN&user=QsQMFZkAAAAJ">Mengdi Zhao</a>, </br>
    <a href="https://kaichen-z.github.io/">Kaichen Zhou</a>,
    <a href="https://scholar.google.com/citations?user=OFdytjoAAAAJ&hl=en">Pheng-Ann Heng</a>,
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>
    <br>
    <em>ICLR</em>, 2026
    <br>
    <a href="https://hybrid-vla.github.io/">project page</a>
    /
    <a href="https://arxiv.org/abs/2503.10631">arXiv</a>
    /
    <a href="https://github.com/PKU-HMI-Lab/Hybrid-VLA">code</a>
    <p></p>
    <p>
      HybridVLA innovatively integrates diffusion and autoregressive action prediction within a single LLM, fully leveraging the continuity and probabilistic nature of diffusion alongside the reasoning capabilities of autoregressive modeling.
    </p>
  </td>
</tr>


<tr onmouseout="h2r_stop()" onmouseover="h2r_start()" bgcolor="#ffffd0">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='h2r_image'>
        <img src='images/h2r_teaser.png' width=100%>
      </div>
      <img src='images/h2r_method.png' width=100%>
    </div>
    <script type="text/javascript">
      function h2r_start() {
        document.getElementById('h2r_image').style.opacity = "0";
      }
      function h2r_stop() {
        document.getElementById('h2r_image').style.opacity = "1";
      }
      h2r_stop()
    </script>
  </td>
  <td style="padding:20px;width:80%;vertical-align:middle">
    <a href="https://sites.google.com/view/h2r-robotics">
      <span class="papertitle">H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos</span>
    </a>
    <br>
    <a href="">Guangrun Li<sup>*</sup></a>,
    <a href="">Yaoxu Lyu<sup>*</sup></a>, 
    <strong>Zhuoyang Liu<sup>*</sup></strong>, 
    <a href="https://scholar.google.com/citations?hl=en&user=iXENYMYAAAAJ">Chengkai Hou<sup>*</sup></a>,
    <a href="https://jieyuz2.github.io/">Jieyu Zhang</a>, 
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>
    <br>
    <em>CVPR workshop</em>, 2025
    <br>
    <a href="https://sites.google.com/view/h2r-robotics">project page</a>
    /
    <a href="https://arxiv.org/abs/2505.11920">arXiv</a>
    /
    <a href="https://huggingface.co/datasets/yaoxu789/H2R-1M">dataset</a>
    <p></p>
    <p>
      H2R is a simple data augmentation technique for robot pre-training from videos, which extracts the human hands from first-person videos and replaces them with those of different robots to generate new video data for pre-training.
    </p>
  </td>
</tr>


          </tbody></table>

          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
